dickens_bigrams <- charles_dickens_collection %>%
group_by(title) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
select(-gutenberg_id, linenumber, chapter)%>%
rename(book = title)
dickens_bigrams
dickens_bigrams <- charles_dickens_collection %>%
group_by(title) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
select(-gutenberg_id, -linenumber, -chapter)%>%
rename(book = title)
dickens_bigrams
dickens_bigrams %>%
count(bigram, sort = TRUE)
dickens_separated <- dickens_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
dickens_separated
dickens_she <- dickens_separated %>%
filter(word1 == "she") %>%
count(book, word2) %>%
rename(word = word2, she_n = n) %>%
select(-book) %>%
group_by(word) %>%
summarize(she_n = sum(she_n)) %>%
filter(she_n > 1) %>%
arrange(desc(she_n))
view(dickens_she)
dickens_he <- dickens_separated %>%
filter(word1 == "he") %>%
count(book, word2) %>%
rename(word = word2, he_n = n) %>%
select(-book) %>%
group_by(word) %>%
summarize(he_n = sum(he_n)) %>%
filter(he_n > 1) %>%
arrange(desc(he_n))
view(dickens_he)
gender_combine <- inner_join(dickens_she, dickens_he) %>%
anti_join(stop_words)  %>%
mutate(verb_ratio_she = she_n / he_n, verb_ratio_he = -1*(he_n / she_n)) %>%
mutate(verb_ratio_she = replace(verb_ratio_she, verb_ratio_she < 1, 0)) %>%
mutate(verb_ratio_he = replace(verb_ratio_he, verb_ratio_he > -1, 0)) %>%
mutate(verb_ratio = verb_ratio_she + verb_ratio_he) %>%
arrange(desc(verb_ratio))
view(gender_combine)
gender_perception <- rbind(head(gender_combine, 15), tail(gender_combine, 15))
view(gender_perception)
gender_perception %>%
ggplot(aes(fct_reorder(word, verb_ratio), verb_ratio)) +
geom_col(show.legend =FALSE) +
coord_flip()
gender_perception <- rbind(head(gender_combine, 15), tail(gender_combine, 15))
view(gender_perception)
gender_perception %>%
ggplot(aes(fct_reorder(word, verb_ratio), verb_ratio)) +
geom_col(show.legend =FALSE) +
coord_flip()
titles <- c("Emma",
"Great Expectations",
"Alice's Adventures in Wonderland",
"The War of the Worlds")
books <- gutenberg_works(title %in% titles) %>%
gutenberg_download(meta_fields = "title")
books
unique(books$title)
# I've TORN THE BOOKS APART!!!
by_chapter <- books %>%
group_by(title) %>%
mutate(chapter = cumsum(str_detect(text,
regex("^chapter ",
ignore_case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0) %>%
unite(document, title, chapter)
by_chapter
# Let's use topic modeling to see if we can put books back together
# As a first step, let's tokenize and tidy these chapters.
word_counts <- by_chapter %>%
unnest_tokens(word, text) %>%
anti_join(get_stopwords(source = "smart")) %>%
count(document, word, sort = TRUE) %>%
group_by(word) %>%
filter(n() > 10) %>%
ungroup()
word_counts
# Next, let's **cast** to a sparse matrix.
words_sparse <- word_counts %>%
cast_sparse(document, word, n)
class(words_sparse)
# Train a topic model: written in C++, main function is stm and is Silge's favorite package. STM is
# an unsupervised machine learning model. K identifies K topics within the corpus.
topic_model <- stm(words_sparse, K = 4,
init.type = "Spectral")
summary(topic_model)
## Explore the output of topic modeling
# The word-topic probabilities are called the "beta" matrix. This creates the probability that a
# given word appears in a topic.
chapter_topics <- tidy(topic_model, matrix = "beta")
chapter_topics
# What are the highest probability words in each topic?
top_terms <- chapter_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
#top_terms
#view(top_terms)
# Let's build a visualization.
top_terms %>%
mutate(term = fct_reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
# The document-topic probabilities are called "gamma". Looks at every document and assigns a
# probability that a given topic was generated from a specific document
chapters_gamma <- tidy(topic_model, matrix = "gamma",
document_names = rownames(words_sparse))
unique(chapters_gamma$document)
## How well did we do in putting our books back together into the 4 topics?
chapters_parsed <- chapters_gamma %>%
separate(document, c("title", "chapter"),
sep = "_", convert = TRUE)
chapters_parsed
# Let's visualization the results. This is  agraphical representation showing is that the algorithm
# learned what book generates what topic
chapters_parsed %>%
mutate(title = fct_reorder(title, gamma * topic)) %>%
ggplot(aes(factor(topic), gamma)) +
geom_boxplot() +
facet_wrap(~ title)
library(tidyverse)
library(tidytext)
library(ggplot2)
library(gutenbergr)
library(stm)
library(glmnet)
library(yardstick)
library(broom)
titles <- c("Emma",
"Great Expectations",
"Alice's Adventures in Wonderland",
"The War of the Worlds")
books <- gutenberg_works(title %in% titles) %>%
gutenberg_download(meta_fields = "title")
books
unique(books$title)
# I've TORN THE BOOKS APART!!!
by_chapter <- books %>%
group_by(title) %>%
mutate(chapter = cumsum(str_detect(text,
regex("^chapter ",
ignore_case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0) %>%
unite(document, title, chapter)
by_chapter
# Let's use topic modeling to see if we can put books back together
# As a first step, let's tokenize and tidy these chapters.
word_counts <- by_chapter %>%
unnest_tokens(word, text) %>%
anti_join(get_stopwords(source = "smart")) %>%
count(document, word, sort = TRUE) %>%
group_by(word) %>%
filter(n() > 10) %>%
ungroup()
word_counts
# Next, let's **cast** to a sparse matrix.
words_sparse <- word_counts %>%
cast_sparse(document, word, n)
class(words_sparse)
# Train a topic model: written in C++, main function is stm and is Silge's favorite package. STM is
# an unsupervised machine learning model. K identifies K topics within the corpus.
topic_model <- stm(words_sparse, K = 4,
init.type = "Spectral")
summary(topic_model)
## Explore the output of topic modeling
# The word-topic probabilities are called the "beta" matrix. This creates the probability that a
# given word appears in a topic.
chapter_topics <- tidy(topic_model, matrix = "beta")
chapter_topics
# What are the highest probability words in each topic?
top_terms <- chapter_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
#top_terms
#view(top_terms)
# Let's build a visualization.
top_terms %>%
mutate(term = fct_reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
# The document-topic probabilities are called "gamma". Looks at every document and assigns a
# probability that a given topic was generated from a specific document
chapters_gamma <- tidy(topic_model, matrix = "gamma",
document_names = rownames(words_sparse))
unique(chapters_gamma$document)
## How well did we do in putting our books back together into the 4 topics?
chapters_parsed <- chapters_gamma %>%
separate(document, c("title", "chapter"),
sep = "_", convert = TRUE)
chapters_parsed
# Let's visualization the results. This is  agraphical representation showing is that the algorithm
# learned what book generates what topic
chapters_parsed %>%
mutate(title = fct_reorder(title, gamma * topic)) %>%
ggplot(aes(factor(topic), gamma)) +
geom_boxplot() +
facet_wrap(~ title)
titles <- c("Emma",
"A Modest Proposal",
"Alice's Adventures in Wonderland",
"Little Women")
books <- gutenberg_works(title %in% titles) %>%
gutenberg_download(meta_fields = "title")
books
unique(books$title)
# I've TORN THE BOOKS APART!!!
by_chapter <- books %>%
group_by(title) %>%
mutate(chapter = cumsum(str_detect(text,
regex("^chapter ",
ignore_case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0) %>%
unite(document, title, chapter)
by_chapter
# Let's use topic modeling to see if we can put books back together
# As a first step, let's tokenize and tidy these chapters.
word_counts <- by_chapter %>%
unnest_tokens(word, text) %>%
anti_join(get_stopwords(source = "smart")) %>%
count(document, word, sort = TRUE) %>%
group_by(word) %>%
filter(n() > 10) %>%
ungroup()
word_counts
# Next, let's **cast** to a sparse matrix.
words_sparse <- word_counts %>%
cast_sparse(document, word, n)
class(words_sparse)
# Train a topic model: written in C++, main function is stm and is Silge's favorite package. STM is
# an unsupervised machine learning model. K identifies K topics within the corpus.
topic_model <- stm(words_sparse, K = 4,
init.type = "Spectral")
summary(topic_model)
## Explore the output of topic modeling
# The word-topic probabilities are called the "beta" matrix. This creates the probability that a
# given word appears in a topic.
chapter_topics <- tidy(topic_model, matrix = "beta")
chapter_topics
# What are the highest probability words in each topic?
top_terms <- chapter_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
#top_terms
#view(top_terms)
# Let's build a visualization.
top_terms %>%
mutate(term = fct_reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
# The document-topic probabilities are called "gamma". Looks at every document and assigns a
# probability that a given topic was generated from a specific document
chapters_gamma <- tidy(topic_model, matrix = "gamma",
document_names = rownames(words_sparse))
unique(chapters_gamma$document)
## How well did we do in putting our books back together into the 4 topics?
chapters_parsed <- chapters_gamma %>%
separate(document, c("title", "chapter"),
sep = "_", convert = TRUE)
chapters_parsed
# Let's visualization the results. This is  agraphical representation showing is that the algorithm
# learned what book generates what topic
chapters_parsed %>%
mutate(title = fct_reorder(title, gamma * topic)) %>%
ggplot(aes(factor(topic), gamma)) +
geom_boxplot() +
facet_wrap(~ title)
titles <- c("Emma",
"Siddhartha",
"Alice's Adventures in Wonderland",
"Little Women")
books <- gutenberg_works(title %in% titles) %>%
gutenberg_download(meta_fields = "title")
books
unique(books$title)
by_chapter <- books %>%
group_by(title) %>%
mutate(chapter = cumsum(str_detect(text,
regex("^chapter ",
ignore_case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0) %>%
unite(document, title, chapter)
by_chapter
# Let's use topic modeling to see if we can put books back together
# As a first step, let's tokenize and tidy these chapters.
word_counts <- by_chapter %>%
unnest_tokens(word, text) %>%
anti_join(get_stopwords(source = "smart")) %>%
count(document, word, sort = TRUE) %>%
group_by(word) %>%
filter(n() > 10) %>%
ungroup()
word_counts
# Next, let's **cast** to a sparse matrix.
words_sparse <- word_counts %>%
cast_sparse(document, word, n)
class(words_sparse)
# Train a topic model: written in C++, main function is stm and is Silge's favorite package. STM is
# an unsupervised machine learning model. K identifies K topics within the corpus.
topic_model <- stm(words_sparse, K = 4,
init.type = "Spectral")
summary(topic_model)
## Explore the output of topic modeling
# The word-topic probabilities are called the "beta" matrix. This creates the probability that a
# given word appears in a topic.
chapter_topics <- tidy(topic_model, matrix = "beta")
chapter_topics
# What are the highest probability words in each topic?
top_terms <- chapter_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
#top_terms
#view(top_terms)
# Let's build a visualization.
top_terms %>%
mutate(term = fct_reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
# The document-topic probabilities are called "gamma". Looks at every document and assigns a
# probability that a given topic was generated from a specific document
chapters_gamma <- tidy(topic_model, matrix = "gamma",
document_names = rownames(words_sparse))
unique(chapters_gamma$document)
## How well did we do in putting our books back together into the 4 topics?
chapters_parsed <- chapters_gamma %>%
separate(document, c("title", "chapter"),
sep = "_", convert = TRUE)
chapters_parsed
# Let's visualization the results. This is  agraphical representation showing is that the algorithm
# learned what book generates what topic
chapters_parsed %>%
mutate(title = fct_reorder(title, gamma * topic)) %>%
ggplot(aes(factor(topic), gamma)) +
geom_boxplot() +
facet_wrap(~ title)
titles <- c("Emma",
"The Hound of the Baskervilles",
"Alice's Adventures in Wonderland",
"Little Women")
books <- gutenberg_works(title %in% titles) %>%
gutenberg_download(meta_fields = "title")
books
unique(books$title)
by_chapter <- books %>%
group_by(title) %>%
mutate(chapter = cumsum(str_detect(text,
regex("^chapter ",
ignore_case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0) %>%
unite(document, title, chapter)
by_chapter
# Let's use topic modeling to see if we can put books back together
# As a first step, let's tokenize and tidy these chapters.
word_counts <- by_chapter %>%
unnest_tokens(word, text) %>%
anti_join(get_stopwords(source = "smart")) %>%
count(document, word, sort = TRUE) %>%
group_by(word) %>%
filter(n() > 10) %>%
ungroup()
word_counts
# Next, let's **cast** to a sparse matrix.
words_sparse <- word_counts %>%
cast_sparse(document, word, n)
class(words_sparse)
# Train a topic model: written in C++, main function is stm and is Silge's favorite package. STM is
# an unsupervised machine learning model. K identifies K topics within the corpus.
topic_model <- stm(words_sparse, K = 4,
init.type = "Spectral")
summary(topic_model)
## Explore the output of topic modeling
# The word-topic probabilities are called the "beta" matrix. This creates the probability that a
# given word appears in a topic.
chapter_topics <- tidy(topic_model, matrix = "beta")
chapter_topics
# What are the highest probability words in each topic?
top_terms <- chapter_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
#top_terms
#view(top_terms)
# Let's build a visualization.
top_terms %>%
mutate(term = fct_reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
# The document-topic probabilities are called "gamma". Looks at every document and assigns a
# probability that a given topic was generated from a specific document
chapters_gamma <- tidy(topic_model, matrix = "gamma",
document_names = rownames(words_sparse))
unique(chapters_gamma$document)
## How well did we do in putting our books back together into the 4 topics?
chapters_parsed <- chapters_gamma %>%
separate(document, c("title", "chapter"),
sep = "_", convert = TRUE)
chapters_parsed
# Let's visualization the results. This is  agraphical representation showing is that the algorithm
# learned what book generates what topic
chapters_parsed %>%
mutate(title = fct_reorder(title, gamma * topic)) %>%
ggplot(aes(factor(topic), gamma)) +
geom_boxplot() +
facet_wrap(~ title)
titles <- c("Emma",
"The Hound of the Baskervilles",
"Alice's Adventures in Wonderland",
"The War of the Worlds")
books <- gutenberg_works(title %in% titles) %>%
gutenberg_download(meta_fields = "title")
books
unique(books$title)
by_chapter <- books %>%
group_by(title) %>%
mutate(chapter = cumsum(str_detect(text,
regex("^chapter ",
ignore_case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0) %>%
unite(document, title, chapter)
by_chapter
# Let's use topic modeling to see if we can put books back together
# As a first step, let's tokenize and tidy these chapters.
word_counts <- by_chapter %>%
unnest_tokens(word, text) %>%
anti_join(get_stopwords(source = "smart")) %>%
count(document, word, sort = TRUE) %>%
group_by(word) %>%
filter(n() > 10) %>%
ungroup()
word_counts
# Next, let's **cast** to a sparse matrix.
words_sparse <- word_counts %>%
cast_sparse(document, word, n)
class(words_sparse)
# Train a topic model: written in C++, main function is stm and is Silge's favorite package. STM is
# an unsupervised machine learning model. K identifies K topics within the corpus.
topic_model <- stm(words_sparse, K = 4,
init.type = "Spectral")
summary(topic_model)
## Explore the output of topic modeling
# The word-topic probabilities are called the "beta" matrix. This creates the probability that a
# given word appears in a topic.
chapter_topics <- tidy(topic_model, matrix = "beta")
chapter_topics
# What are the highest probability words in each topic?
top_terms <- chapter_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
#top_terms
#view(top_terms)
# Let's build a visualization.
top_terms %>%
mutate(term = fct_reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
# The document-topic probabilities are called "gamma". Looks at every document and assigns a
# probability that a given topic was generated from a specific document
chapters_gamma <- tidy(topic_model, matrix = "gamma",
document_names = rownames(words_sparse))
unique(chapters_gamma$document)
## How well did we do in putting our books back together into the 4 topics?
chapters_parsed <- chapters_gamma %>%
separate(document, c("title", "chapter"),
sep = "_", convert = TRUE)
chapters_parsed
# Let's visualization the results. This is  agraphical representation showing is that the algorithm
# learned what book generates what topic
chapters_parsed %>%
mutate(title = fct_reorder(title, gamma * topic)) %>%
ggplot(aes(factor(topic), gamma)) +
geom_boxplot() +
facet_wrap(~ title)
