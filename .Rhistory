# Find *high* tf-idf words.
book_tfidf %>%
arrange(-tf_idf)
# How can we visualize this? Let's go step-by-step.
book_tfidf %>%
group_by(title) %>%
top_n(10) %>%
ungroup %>%
ggplot(aes(fct_reorder(word, tf_idf),
tf_idf,
fill = title)) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~title, scales = "free")
full_collection <- gutenberg_download(c(98, 4300, 844, 135),
meta_fields = "title")
# count the number of words per book
full_collection %>% count(title)
# Count the word frequencies in your collection, by title.
book_words <- full_collection %>%
unnest_tokens(word, text) %>%
count(title, word, sort = TRUE)
book_words
## Calculate tf-idf: term frequency - inverse document frequency. the weight is a statstical measure
## used to evaluate how important a word is to a document in a collection. The importance increases
## proportionally to the number of times a word appears in a document but is offset by the frequency
## of the word in the collection (corpus).
## Calculate tf-idf.
book_tfidf <- book_words %>%
bind_tf_idf(word, title, n)
book_tfidf
# Find *high* tf-idf words.
book_tfidf %>%
arrange(-tf_idf)
# How can we visualize this? Let's go step-by-step.
book_tfidf %>%
group_by(title) %>%
top_n(10) %>%
ungroup %>%
ggplot(aes(fct_reorder(word, tf_idf),
tf_idf,
fill = title)) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~title, scales = "free")
tidy_dickens
book_words
tidy_dickens
book_words <- tidy_dickens %>%
unnest_tokens(word, text) %>%
count(title, word, sort = TRUE)
book_words
book_tfidf <- book_words %>%
bind_tf_idf(word, title, n)
book_tfidf
book_tfidf %>%
arrange(-tf_idf)
book_tfidf <- book_words %>%
bind_tf_idf(word, title, n) %>%
arrange(-tf_idf)
book_tfidf
book_tfidf %>%
group_by(title) %>%
top_n(10) %>%
ungroup %>%
ggplot(aes(fct_reorder(word, tf_idf),
tf_idf,
fill = title)) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~title, scales = "free")
book_words <- tidy_dickens %>%
unnest_tokens(word, text) %>%
count(title, word, sort = TRUE)
book_words
## Calculate tf-idf: term frequency - inverse document frequency. the weight is a statstical measure
## used to evaluate how important a word is to a document in a collection. The importance increases
## proportionally to the number of times a word appears in a document but is offset by the frequency
## of the word in the collection (corpus).
## Calculate tf-idf and arrange by "high" tf-idf words.
book_tfidf <- book_words %>%
bind_tf_idf(word, title, n) %>%
arrange(-tf_idf)
book_tfidf
# visulaize tf-dif by book.
book_tfidf %>%
group_by(title) %>%
top_n(10) %>%
ungroup %>%
ggplot(aes(fct_reorder(word, tf_idf),
tf_idf,
fill = title)) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~title, scales = "free")
tidy_dickens
unique(tidy_dickens)
tidy_dickens %>%
unique(title)
unique(tidy_dickens$title)
book_words <- tidy_dickens %>%
unnest_tokens(word, text) %>%
count(title, word, sort = TRUE)
tidy_dickens
book_words <- tidy_dickens %>%
count(title, word, sort = TRUE)
book_words
book_tfidf <- book_words %>%
bind_tf_idf(word, title, n) %>%
arrange(-tf_idf)
book_tfidf
book_tfidf %>%
group_by(title) %>%
top_n(10) %>%
ungroup %>%
ggplot(aes(fct_reorder(word, tf_idf),
tf_idf,
fill = title)) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~title, scales = "free")
View(jane_austen_sentiment)
getwd()
library(tidyverse)
library(tidytext)
library(ggplot2)
library(gutenbergr)
library(janeaustenr)
text <- c("If you can dream – and not make dreams your master;",
"If you can think – and not make thoughts your aim;",
"If you can meet with Triumph and Disaster",
"And treat those two impostors just the same;",
"If you can bear to hear the truth you’ve spoken",
"Twisted by knaves to make a trap for fools,",
"Or watch the things you gave your life to, broken,",
"And stoop and build ’em up with worn-out tools:")
text
text_df <- data_frame(line = 1:8, text = text)
text_df
text_df %>%
unnest_tokens(word, text)
full_text <- gutenberg_download(135)
tidy_book <- full_text %>%
mutate(line = row_number()) %>%
unnest_tokens(word, text)
tidy_book
tidy_book %>%
count(word, sort = TRUE) %>%
top_n(20) %>%
ggplot(aes(fct_reorder(word, n), n)) +
geom_col() +
coord_flip()
tidy_book %>%
count(word, sort = TRUE)
tidy_book %>%
count(word, sort = TRUE) %>%
summarise(sum(n))
library(tidyverse)
library(tidytext)
library(ggplot2)
library(gutenbergr)
library(janeaustenr)
text <- c("If you can dream – and not make dreams your master;",
"If you can think – and not make thoughts your aim;",
"If you can meet with Triumph and Disaster",
"And treat those two impostors just the same;",
"If you can bear to hear the truth you’ve spoken",
"Twisted by knaves to make a trap for fools,",
"Or watch the things you gave your life to, broken,",
"And stoop and build ’em up with worn-out tools:")
text
# Put text in a data frame, one line per row
text_df <- data_frame(line = 1:8, text = text)
text_df
# what we want is one word per row. unnest_tokens breaks out words
text_df %>%
unnest_tokens(word, text)
## What are a texts most common words?
# Load text. Use Les Miserables, by Victor Hugo - gutenberg library code 135
full_text <- gutenberg_download(135)
tidy_book <- full_text %>%
mutate(line = row_number()) %>%
unnest_tokens(word, text)
tidy_book
# What are the most common words?
tidy_book %>%
count(word, sort = TRUE) %>%
#  summarise(sum(n))
top_n(20) %>%
ggplot(aes(fct_reorder(word, n), n)) +
geom_col() +
coord_flip()
# This isn't terribly useful. Filter using stopword lexicons to get a better analysis
# stop_words combines stopwords from three lexicons or
# use get_stopwords() to filter by a specific lexicon
get_stopwords()
view(stop_words)
# Another try at most common words, use the 'smart' lexicon
tidy_book %>%
#  anti_join(get_stopwords(source = "smart")) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE) %>%
top_n(20) %>%
ggplot(aes(fct_reorder(word, n), n)) +
geom_col() +
coord_flip()
### Sentiment Analyses ###
# Tidytext containts three general purpose sentiment lexicons - based on unigrams.
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
## tidy the data
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
## What are the most commonly used words in jane Austen's Novels"
#
# Total Words
#total_words <- tidy_books %>%
#  group_by(book) %>%
#  count(book, word, sort = TRUE) %>%
#  summarize(total = sum(n)) %>%
#  ungroup()
#
# Filtered Total Words
#book_words <- tidy_books %>%
#  anti_join(stop_words) %>%
#  count(book, word, sort = TRUE)
#
#total_words <- book_words %>%
#  group_by(book) %>%
#  summarize(total = sum(n))
#
#book_words <- left_join(book_words, total_words)
#
#ggplot(book_words, words, aes(n/total, fill = book)) +
#  geom_histogram(show.legend = FALSE) +
#  xlim(NA, 0.0009) +
#  facet_wrap(~book, ncol = 2, scales = "free_y")
#
## build a word count using the "joy" sentiment
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE)
## Build a positive and negative word count for the book "Emma"
tidy_books %>%
filter(book == "Emma") %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment, word, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup %>%
ggplot(aes(fct_reorder(word, n), n, fill = sentiment)) +
geom_col() +
coord_flip() +
facet_wrap(~ sentiment, scales = "free")
# "miss" is a misleading word. It is not necessarily negative but instead a title for an
# unamrried woman. Let's add it as a custom stopword and recreate the count.
custom_stop_words <- tibble(word = c("miss"), lexicon = c("custom"))
# Now recount and replot
tidy_books %>%
filter(book == "Emma") %>%
inner_join(get_sentiments("bing")) %>%
anti_join(custom_stop_words) %>%
count(sentiment, word, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup %>%
ggplot(aes(fct_reorder(word, n), n, fill = sentiment)) +
geom_col() +
coord_flip() +
facet_wrap(~ sentiment, scales = "free")
## build a sentiment score
# Upload and create a new corpus of books
# Gutenebrg ID: 1400 - Great Expectations
# Gutenebrg ID: 98 - A Tale of Two Cities
# Gutenebrg ID: 1023 - Bleak House
# Gutenebrg ID: 24022 - A Christmas Carol
# Gutenebrg ID: 766 - David Copperfield
# Gutenebrg ID: 730 - Oliver Twist
charles_dickens_collection <- gutenberg_download(c(1400, 98, 1023, 24022, 766, 730),
meta_fields = "title")
charles_dickens_collection %>% count(title)
# Tidy the corpus
tidy_dickens <- charles_dickens_collection %>%
group_by(title) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
charles_dickens_sentiment <- tidy_dickens %>%
inner_join(get_sentiments("bing")) %>%
count(title, index = linenumber %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
ggplot(charles_dickens_sentiment, aes(index, sentiment, fill = title)) +
geom_col(show.legend = FALSE) +
facet_wrap(~title, ncol = 2, scales = "free_x")
## Which sentiment lexicon works best? The one that is most appropriate for your purpose!
# AFINN assigns words with a score that runs between -5 and 5, with negative scores indicating
# negative sentiment and positive scores indicating positive sentiment
# bing categorizes words in a binary fashion into positive and negative categories
# nrc categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative,
# anger, anticipation, disgust, fear, joy, sadness, surprise, and trust
# Compare Sentiment Scores of " A Tale of Two Cities" using AFINN, bing, and nrc
two_cities <- tidy_dickens %>%
filter(title == "A Tale of Two Cities")
two_cities
# NOTE: AFINN lexicon measures sentiment with a numeric score between -5 and 5, while the other two
# lexicons categorize words in a binary fashion, either positive or negative. To find a sentiment
# score in chunks of text throughout the novel, we will need to use a different pattern for the
# AFINN lexicon than for the other two.
afinn <- two_cities %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index = linenumber %/% 80) %>%
summarise(sentiment = sum(score)) %>%
mutate(method = "AFINN")
bing_and_nrc <- bind_rows(two_cities %>%
inner_join(get_sentiments("bing")) %>%
mutate(method = "Bing et al."), two_cities %>%
inner_join(get_sentiments("nrc") %>%
filter(sentiment %in% c("positive", "negative"))) %>%
mutate(method = "NRC")) %>%
count(method, index = linenumber %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
bind_rows(afinn, bing_and_nrc) %>%
ggplot(aes(index, sentiment, fill = method)) +
geom_col(show.legend = FALSE) +
facet_wrap(~method, ncol = 1, scales = "free_y")
# Calculate the tf-idf for the Charles Dickens Corpus
tidy_dickens
unique(tidy_dickens$title)
book_words <- tidy_dickens %>%
count(title, word, sort = TRUE)
book_words
## Calculate tf-idf: term frequency - inverse document frequency. the weight is a statstical measure
## used to evaluate how important a word is to a document in a collection. The importance increases
## proportionally to the number of times a word appears in a document but is offset by the frequency
## of the word in the collection (corpus).
## Calculate tf-idf and arrange by "high" tf-idf words.
book_tfidf <- book_words %>%
bind_tf_idf(word, title, n) %>%
arrange(-tf_idf)
book_tfidf
# visulaize tf-dif by book.
book_tfidf %>%
group_by(title) %>%
top_n(10) %>%
ungroup %>%
ggplot(aes(fct_reorder(word, tf_idf),
tf_idf,
fill = title)) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~title, scales = "free")
austen_bigrams <- austen_books() %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
austen_bigrams
austen_bigrams %>%
count(bigram, sort = TRUE)
bigrams_separated <- austen_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_separated
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
bigrams_filtered
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigram_counts
bigrams_filtered %>%
filter(word2 == "street") %>%
count(book, word1, sort = TRUE)
austen_bigrams %>%
count(bigram, sort = TRUE)
austen_separated <- austen_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
austen_separated
austen_she <- austen_separated %>%
filter(word1 == "she") %>%
count(book, word2) %>%
rename(word = word2, she_n = n) %>%
select(-book) %>%
group_by(word) %>%
summarize(she_n = sum(she_n)) %>%
filter(she_n > 1) %>%
arrange(desc(she_n))
view(austen_she)
austen_he <- austen_separated %>%
filter(word1 == "he") %>%
count(book, word2) %>%
rename(word = word2, he_n = n) %>%
select(-book) %>%
group_by(word) %>%
summarize(he_n = sum(he_n)) %>%
filter(he_n > 1) %>%
arrange(desc(he_n))
view(austen_he)
gender_combine <- inner_join(austen_she, austen_he) %>%
anti_join(stop_words)  %>%
mutate(verb_ratio = she_n / he_n) %>%
arrange(desc(verb_ratio))
gender_combine
head(gender_combine, 15)
tail(gender_combine, 15)
gender_perception <- rowbind(head(gender_combine, 15), tail(gender_combine, 15))
head <- head(gender_combine, 15)
tail <- tail(gender_combine, 15)
head
tail
gender_perception <- rowbind(head, tail)
gender_perception <- colbind(head, tail)
gender_perception <- rbind(head, tail)
gender_perception
view(gender_perception)
gender_perception <- rbind(head(gender_combine, 15), tail(gender_combine, 15))
gender_perception
view(gender_perception)
gender_perception %>%
ggplot() +
geom_bar() +
coord_flip()
gender_perception %>%
ggplot() +
geom_col() +
coord_flip()
book_tfidf
gender_perception
gender_perception %>%
ggplot(aes(word, verb_ratio)) +
geom_col(show.legend =FALSE) +
coord_flip()
gender_perception
gender_combine %>%
filter(head(15), tail(15)) %>%
ggplot(aes(word, verb_ratio)) +
geom_col(show.legend =FALSE) +
coord_flip()
view(gender_perception)
gender_combine %>%
filter(gender_combine >= 3.5 , gender_combine <= 0.6) %>%
ggplot(aes(word, verb_ratio)) +
geom_col(show.legend =FALSE) +
coord_flip()
gender_combine %>%
filter(verb_ratio >= 3.5 , verb_ratio <= 0.6) %>%
ggplot(aes(word, verb_ratio)) +
geom_col(show.legend =FALSE) +
coord_flip()
gender_combine %>%
filter(verb_ratio >= 3.5 & verb_ratio <= 0.6) %>%
ggplot(aes(word, verb_ratio)) +
geom_col(show.legend =FALSE) +
coord_flip()
gender_combine %>%
filter(verb_ratio >= 3.5 | verb_ratio <= 0.6) %>%
ggplot(aes(word, verb_ratio)) +
geom_col(show.legend =FALSE) +
coord_flip()
gender_combine %>%
filter(verb_ratio >= 3.5 | verb_ratio <= 0.6) %>%
ggplot(aes(fct_reorder(word, verb_ratio), word, verb_ratio)) +
geom_col(show.legend =FALSE) +
coord_flip()
gender_combine %>%
filter(verb_ratio >= 3.5 | verb_ratio <= 0.6) %>%
ggplot(aes(fct_reorder(verb_ratio, word), word, verb_ratio)) +
geom_col(show.legend =FALSE) +
coord_flip()
gender_combine %>%
filter(verb_ratio >= 3.5 | verb_ratio <= 0.6) %>%
ggplot(aes(fct_reorder(word, verb_ratio), word, verb_ratio)) +
geom_col(show.legend =FALSE) +
coord_flip()
gender_combine %>%
filter(verb_ratio >= 3.5 | verb_ratio <= 0.6) %>%
ggplot(aes(fct_reorder(word, verb_ratio), verb_ratio)) +
geom_col(show.legend =FALSE) +
coord_flip()
cut.off = 1
gender_combine %>%
filter(verb_ratio >= 3.5 | verb_ratio <= 0.6) %>%
ggplot(aes(fct_reorder(word, verb_ratio), verb_ratio)) +
geom_col(show.legend =FALSE) +
coord_flip()
gender_combine %>%
filter(verb_ratio >= 3.5 | verb_ratio <= 0.6) %>%
mutate(verb_ratio = verb_ratio - 1)
ggplot(aes(fct_reorder(word, verb_ratio), verb_ratio)) +
geom_col(show.legend =FALSE) +
coord_flip()
gender_combine %>%
filter(verb_ratio >= 3.5 | verb_ratio <= 0.6) %>%
mutate(verb_ratio = verb_ratio - 1) %>%
ggplot(aes(fct_reorder(word, verb_ratio), verb_ratio)) +
geom_col(show.legend =FALSE) +
coord_flip()
gender_combine
gender_perception
gender_combine %>%
filter(head(gender_combine, 15) | tail(gender_combine, 15)) %>%
mutate(verb_ratio = verb_ratio - 1) %>%
ggplot(aes(fct_reorder(word, verb_ratio), verb_ratio)) +
geom_col(show.legend =FALSE) +
coord_flip()
