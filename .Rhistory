anti_join(get_stopwords(source = "smart")) %>%
count(document, word, sort = TRUE) %>%
group_by(word) %>%
filter(n() > 10) %>%
ungroup()
word_counts
# Next, let's **cast** to a sparse matrix.
words_sparse <- word_counts %>%
cast_sparse(document, word, n)
class(words_sparse)
# Train a topic model: written in C++, main function is stm and is Silge's favorite package. STM is
# an unsupervised machine learning model. K identifies K topics within the corpus.
topic_model <- stm(words_sparse, K = 6,
init.type = "Spectral")
summary(topic_model)
## Explore the output of topic modeling
# The word-topic probabilities are called the "beta" matrix. This creates the probability that a
# given word appears in a topic.
chapter_topics <- tidy(topic_model, matrix = "beta")
chapter_topics
# What are the highest probability words in each topic?
top_terms <- chapter_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
#top_terms
#view(top_terms)
# Let's build a visualization.
top_terms %>%
mutate(term = fct_reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
# The document-topic probabilities are called "gamma". Looks at every document and assigns a
# probability that a given topic was generated from a specific document
chapters_gamma <- tidy(topic_model, matrix = "gamma",
document_names = rownames(words_sparse))
unique(chapters_gamma$document)
## How well did we do in putting our books back together into the 4 topics?
chapters_parsed <- chapters_gamma %>%
separate(document, c("title", "chapter"),
sep = "_", convert = TRUE)
chapters_parsed
# Let's visualization the results. This is  agraphical representation showing is that the algorithm
# learned what book generates what topic
chapters_parsed %>%
mutate(title = fct_reorder(title, gamma * topic)) %>%
ggplot(aes(factor(topic), gamma)) +
geom_boxplot() +
facet_wrap(~ title)
head(chapter_topics,20)
head(chapter_topics,18)
top_terms %>%
mutate(term = fct_reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
arrange(desc(term)) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms
top_terms %>%
mutate(term = fct_reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
arrange(desc(beta)) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
?geom_col
top_terms %>%
mutate(term = fct_reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~ topic, scales = "free")
book_tfidf %>%
group_by(title) %>%
top_n(10) %>%
ungroup %>%
ggplot(aes(fct_reorder(word, tf_idf),
tf_idf,
fill = title)) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~title, scales = "free")
summary(topic_model)
full_text <- gutenberg_download(135)
tidy_book <- full_text %>%
mutate(line = row_number()) %>%
unnest_tokens(word, text)
tidy_book
# What are the most common words?
tidy_book %>%
count(word, sort = TRUE) %>%
#  summarise(sum(n))
top_n(20) %>%
ggplot(aes(fct_reorder(word, n), n)) +
geom_col() +
coord_flip()
# This isn't terribly useful. Filter using stopword lexicons to get a better analysis
# stop_words combines stopwords from three lexicons or
# use get_stopwords() to filter by a specific lexicon
get_stopwords()
view(stop_words)
# Another try at most common words, use the 'smart' lexicon
tidy_book %>%
#  anti_join(get_stopwords(source = "smart")) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE) %>%
top_n(20) %>%
ggplot(aes(fct_reorder(word, n), n)) +
geom_col() +
coord_flip()
### Sentiment Analyses ###
# Tidytext containts three general purpose sentiment lexicons - based on unigrams.
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
## tidy the data
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
## What are the most commonly used words in jane Austen's Novels"
#
# Total Words
#total_words <- tidy_books %>%
#  group_by(book) %>%
#  count(book, word, sort = TRUE) %>%
#  summarize(total = sum(n)) %>%
#  ungroup()
#
# Filtered Total Words
#book_words <- tidy_books %>%
#  anti_join(stop_words) %>%
#  count(book, word, sort = TRUE)
#
#total_words <- book_words %>%
#  group_by(book) %>%
#  summarize(total = sum(n))
#
#book_words <- left_join(book_words, total_words)
#
#ggplot(book_words, words, aes(n/total, fill = book)) +
#  geom_histogram(show.legend = FALSE) +
#  xlim(NA, 0.0009) +
#  facet_wrap(~book, ncol = 2, scales = "free_y")
#
## build a word count using the "joy" sentiment
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE)
## Build a positive and negative word count for the book "Emma"
tidy_books %>%
filter(book == "Emma") %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment, word, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup %>%
ggplot(aes(fct_reorder(word, n), n, fill = sentiment)) +
geom_col() +
expand_limits(y = c(0,600)) +
coord_flip() +
facet_wrap(~ sentiment, scales = "free")
custom_stop_words <- tibble(word = c("miss"), lexicon = c("custom"))
# Now recount and replot
tidy_books %>%
filter(book == "Emma") %>%
inner_join(get_sentiments("bing")) %>%
anti_join(custom_stop_words) %>%
count(sentiment, word, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup %>%
ggplot(aes(fct_reorder(word, n), n, fill = sentiment)) +
geom_col() +
expand_limits(y = c(0,400)) +
coord_flip() +
facet_wrap(~ sentiment, scales = "free")
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE)
?nrc
??nrc
get_sentiments("nrc")
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE)
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
ggplot(aes(fct_reorder(word, n), n, fill = sentiment)) +
geom_col() +
#  expand_limits(y = c(0,600)) +
coord_flip()
tidy_books
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy)
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
ggplot(aes(fct_reorder(word, n), n)) +
geom_col() +
#  expand_limits(y = c(0,600)) +
coord_flip()
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
ggplot(aes(fct_reorder(word, n), n)) +
geom_col() +
#  expand_limits(y = c(0,600)) +
coord_flip()
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
ggplot(aes(fct_reorder(word, n), n)) +
geom_col() +
expand_limits(y = c(0,400)) +
coord_flip()
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
ggplot(aes(fct_reorder(word, n), n, fill = "cyan")) +
geom_col() +
expand_limits(y = c(0,400)) +
coord_flip()
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
ggplot(aes(fct_reorder(word, n), n, fill = "blue")) +
geom_col() +
expand_limits(y = c(0,400)) +
coord_flip()
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
ggplot(aes(fct_reorder(word, n), n, color = "cyan")) +
geom_col() +
expand_limits(y = c(0,400)) +
coord_flip()
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
ggplot(aes(fct_reorder(word, n), n, fill = cyan)) +
geom_col() +
expand_limits(y = c(0,400)) +
coord_flip()
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
ggplot(aes(fct_reorder(word, n), n, fill = color)) +
geom_col() +
expand_limits(y = c(0,400)) +
coord_flip()
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
ggplot(aes(fct_reorder(word, n), n, fill = "white")) +
geom_col() +
expand_limits(y = c(0,400)) +
coord_flip()
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
ggplot(aes(fct_reorder(word, n), n)) +
geom_col(fill = "white") +
expand_limits(y = c(0,400)) +
coord_flip()
library(scales)
show_col(hue_pal()(4))
library(scales)
show_col(hue_pal()(3))
4
library(scales)
show_col(hue_pal()(4))
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
ggplot(aes(fct_reorder(word, n), n)) +
geom_col(fill = "#00BFC4") +
expand_limits(y = c(0,400)) +
coord_flip()
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
top_n(10)
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "trust")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
ggplot(aes(fct_reorder(word, n), n)) +
geom_col(fill = "#00BFC4") +
expand_limits(y = c(0,400)) +
coord_flip()
afinn <- two_cities %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index = linenumber %/% 80) %>%
summarise(sentiment = sum(score)) %>%
mutate(method = "AFINN")
bing_and_nrc <- bind_rows(two_cities %>%
inner_join(get_sentiments("bing")) %>%
mutate(method = "Bing et al."), two_cities %>%
inner_join(get_sentiments("nrc") %>%
filter(sentiment %in% c("positive", "negative"))) %>%
mutate(method = "NRC")) %>%
count(method, index = linenumber %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
bind_rows(afinn, bing_and_nrc) %>%
ggplot(aes(index, sentiment, fill = method)) +
geom_col(show.legend = FALSE) +
facet_wrap(~method, ncol = 1, scales = "free_y")
afinn <- two_cities %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index = linenumber %/% 80) %>%
summarise(sentiment = sum(score)) %>%
mutate(method = "AFINN")
two_cities <- tidy_dickens %>%
filter(title == "A Tale of Two Cities")
two_cities
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
## What are the most commonly used words in jane Austen's Novels"
#
# Total Words
#total_words <- tidy_books %>%
#  group_by(book) %>%
#  count(book, word, sort = TRUE) %>%
#  summarize(total = sum(n)) %>%
#  ungroup()
#
# Filtered Total Words
#book_words <- tidy_books %>%
#  anti_join(stop_words) %>%
#  count(book, word, sort = TRUE)
#
#total_words <- book_words %>%
#  group_by(book) %>%
#  summarize(total = sum(n))
#
#book_words <- left_join(book_words, total_words)
#
#ggplot(book_words, words, aes(n/total, fill = book)) +
#  geom_histogram(show.legend = FALSE) +
#  xlim(NA, 0.0009) +
#  facet_wrap(~book, ncol = 2, scales = "free_y")
## Code that finds colors with the given number of graphs!!
#library(scales)
#show_col(hue_pal()(4))
## build a word count using the "joy" sentiment
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
ggplot(aes(fct_reorder(word, n), n)) +
geom_col(fill = "#00BFC4") +
expand_limits(y = c(0,400)) +
coord_flip()
## build a word count using the "trust" sentiment
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "trust")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
ggplot(aes(fct_reorder(word, n), n)) +
geom_col(fill = "#00BFC4") +
expand_limits(y = c(0,400)) +
coord_flip()
## Build a positive and negative word count for the book "Emma"
tidy_books %>%
filter(book == "Emma") %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment, word, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup %>%
ggplot(aes(fct_reorder(word, n), n, fill = sentiment)) +
geom_col() +
expand_limits(y = c(0,600)) +
coord_flip() +
facet_wrap(~ sentiment, scales = "free")
# "miss" is a misleading word. It is not necessarily negative but instead a title for an
# unamrried woman. Let's add it as a custom stopword and recreate the count.
custom_stop_words <- tibble(word = c("miss"), lexicon = c("custom"))
# Now recount and replot
tidy_books %>%
filter(book == "Emma") %>%
inner_join(get_sentiments("bing")) %>%
anti_join(custom_stop_words) %>%
count(sentiment, word, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup %>%
ggplot(aes(fct_reorder(word, n), n, fill = sentiment)) +
geom_col() +
expand_limits(y = c(0,400)) +
coord_flip() +
facet_wrap(~ sentiment, scales = "free")
## build a sentiment score
# Upload and create a new corpus of books
# Gutenebrg ID: 1400 - Great Expectations
# Gutenebrg ID: 98 - A Tale of Two Cities
# Gutenebrg ID: 1023 - Bleak House
# Gutenebrg ID: 24022 - A Christmas Carol
# Gutenebrg ID: 766 - David Copperfield
# Gutenebrg ID: 730 - Oliver Twist
charles_dickens_collection <- gutenberg_download(c(1400, 98, 1023, 24022, 766, 730),
meta_fields = "title")
charles_dickens_collection %>% count(title)
# Tidy the corpus
tidy_dickens <- charles_dickens_collection %>%
group_by(title) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
charles_dickens_sentiment <- tidy_dickens %>%
inner_join(get_sentiments("bing")) %>%
count(title, index = linenumber %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
ggplot(charles_dickens_sentiment, aes(index, sentiment, fill = title)) +
geom_col(show.legend = FALSE) +
facet_wrap(~title, ncol = 2, scales = "free_x")
## Which sentiment lexicon works best? The one that is most appropriate for your purpose!
# AFINN assigns words with a score that runs between -5 and 5, with negative scores indicating
# negative sentiment and positive scores indicating positive sentiment
# bing categorizes words in a binary fashion into positive and negative categories
# nrc categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative,
# anger, anticipation, disgust, fear, joy, sadness, surprise, and trust
# Compare Sentiment Scores of " A Tale of Two Cities" using AFINN, bing, and nrc
two_cities <- tidy_dickens %>%
filter(title == "A Tale of Two Cities")
two_cities
# NOTE: AFINN lexicon measures sentiment with a numeric score between -5 and 5, while the other two
# lexicons categorize words in a binary fashion, either positive or negative. To find a sentiment
# score in chunks of text throughout the novel, we will need to use a different pattern for the
# AFINN lexicon than for the other two.
afinn <- two_cities %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index = linenumber %/% 80) %>%
summarise(sentiment = sum(score)) %>%
mutate(method = "AFINN")
bing_and_nrc <- bind_rows(two_cities %>%
inner_join(get_sentiments("bing")) %>%
mutate(method = "Bing et al."), two_cities %>%
inner_join(get_sentiments("nrc") %>%
filter(sentiment %in% c("positive", "negative"))) %>%
mutate(method = "NRC")) %>%
count(method, index = linenumber %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
bind_rows(afinn, bing_and_nrc) %>%
ggplot(aes(index, sentiment, fill = method)) +
geom_col(show.legend = FALSE) +
facet_wrap(~method, ncol = 1, scales = "free_y")
afinn
score
get_sentiments("afinn"
)
get_sentiments("bing")
bing_and_nrc
afinn
